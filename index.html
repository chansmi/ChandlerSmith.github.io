<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chandler Smith</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            font-size: 14px;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            margin-bottom: 40px;
        }
        h1 {
            font-size: 24px;
            font-weight: normal;
        }
        a {
            color: #0366d6;
            text-decoration: none;
        }
        .main-content {
            display: flex;
            gap: 40px;
        }
        .bio {
            flex: 2;
        }
        .photo {
            flex: 1;
        }
        .photo img {
            width: 100%;
            border-radius: 50%;
        }
        .research {
            margin-top: 40px;
        }
        .research-item {
            display: flex;
            align-items: flex-start;
            margin-bottom: 30px;
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 30px;
        }
        .research-image {
            width: 300px; /* Increased from 200px */
            margin-right: 20px;
        }
        .research-content {
            flex: 1;
        }
        h3 {
            margin-top: 0;
            color: #0366d6;
        }
        .arrow {
            color: #0366d6;
        }
        .collapsible-content {
            margin-top: 10px;
        }
        .ongoing-projects {
            margin-top: 40px;
        }
    </style>
</head>
<body>
    <header>
        <h1><strong>Chandler Smith</strong></h1>
    </header>
    <div class="main-content">
        <div class="bio">
            <p>My name is <strong>Chandler Smith</strong>, and I am a Research Engineer at the <a href="https://www.cooperativeai.com/">Cooperative AI Foundation</a> where I conduct technical research on multi-agent systems. I am also a <a href="https://foresight.org/technologies/computation-intelligent-cooperation/">Foresight AI Safety Grant Recipient</a>, working on multi-agent security, steganography, and AI control, including our recent piece, "<a href="https://www.lesswrong.com/posts/smMdYezaC8vuiLjCf/secret-collusion-will-we-know-when-to-unplug-ai">Secret Collusion: Will We Know When to Unplug AI?</a>". Additionally, I consult with <a href="https://www.iqt.org/">IQT</a>'s applied research and technology architect teams on AI, multi-agent systems, and AI infrastructure. Previously, I was a <a href="https://www.matsprogram.org/">MATS</a> scholar collaborating with <a href="https://www.linkedin.com/in/jesse-clifton-45652758/">Jesse Clifton</a> on multi-agent systems research. In the past, I worked as an engineer for <a href="https://www.dimagi.com/">Dimagi</a> on global health and COVID response projects.</p>
            <p>As AI systems become increasingly interconnected and autonomous, they introduce unprecedented risks that extend beyond the safety challenges of individual systems. My research focuses on understanding, measuring, and mitigating these risks via benchmarking, oversight, and security in both LM and MARL environments. </p>
            <p>You can find me on <a href="https://twitter.com/ChandlerDSmith">Twitter</a>, <a href="https://www.linkedin.com/in/chandlerdsmith/">LinkedIn</a>, <a href="https://github.com/chansmi">GitHub</a>, and <a href="mailto:smith.18.chandler@gmail.com">Email</a>.</p>
        </div>
        <div class="photo">
            <img src="images/profile_2.jpeg" alt="Chandler Smith">
        </div>
    </div>
    <div class="research">
        <h2>Research</h2>
        <div class="research-item">
            <img src="images/research_1.png" alt="The Concordia Contest" class="research-image">
            <div class="research-content">
                <h3><a href="https://drive.google.com/file/d/1G8WWPomgJ0Mdhg2Dw8ZJxLowXVrvNsyu/view?usp=sharing">The Concordia Contest: Advancing the Cooperative Intelligence of Language Model Agents</a></h3>
                <p><strong>Authors:</strong> <strong>Chandler Smith</strong>, Rakshit S. Trivedi, Jesse Clifton, Lewis Hammond, Akbir Khan, Marwa Abdulhai, Alexander Sasha Vezhnevets, John P. Agapiou, Edgar A. Duéñez-Guzmán, Jayd Matyas, Danny Karmon, Oliver Slumbers, Minsuk Chang, Dylan Hadfield-Menell, Natasha Jaques, Tim Baarslag, Joel Z. Leibo<br>
                <strong>Conference:</strong> NeurIPS 2024 Competition Track<br>
                <strong>Contest Hosting Site:</strong> <a href="https://www.codabench.org/competitions/3888/">Link</a></p>
                <p>Building on the success of the Melting Pot contest at NeurIPS 2023, which challenged participants to develop multi-agent reinforcement learning agents capable of cooperation in groups, we are excited to propose a new contest centered on cooperation between language model agents in intricate, text-mediated environments. Our goal is to advance research on the cooperative intelligence of such LM agents. Of particular interest are the agents capable of using natural language to effectively cooperate with each other in complex environments, even in the face of challenges such as competing interests, differing values, and potential miscommunication.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_6.png" alt="BetterBench" class="research-image">
            <div class="research-content">
                <h3><a href="https://betterbench.stanford.edu/">BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices</a></h3>
                <p><strong>Authors:</strong> Anka Reuel, Amelia Hardy, <strong>Chandler Smith</strong>, Max Lamparth, Mykel J. Kochenderfer</p>
                <strong>Conference:</strong> NeurIPS 2024 Track Datasets and Benchmarks <strong>Spotlight</strong>
                <p>AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_9.png" alt="MALT: Learning to Reason with Multi-Agent LLM Systems" class="research-image">
            <div class="research-content">
                <h3><a href="https://openreview.net/forum?id=gAek5YWgCB#discussion"> MALT: Learning to Reason with Multi-Agent LLM Systems</a></h3>
                <p><strong>Authors:</strong> Sumeet Ramesh Motwani, <strong>Chandler Smith</strong>, Rocktim Jyoti Das, Markian Rybchuk, Ronald Clark, Philip Torr, Fabio Pizzati, Ivan Laptev, Christian Schroeder de Witt<br>
                <strong>Conference:</strong> Preprint - AAMAS 2025 Submission</p>
                <p>[Preprint - Under Review] Enabling effective collaboration among LLMs is a crucial step towards developing autonomous systems capable of solving complex problems. While LLMs are typically used as zero-shot generators, where humans critique and refine their outputs, the potential for models to work together is largely unexplored. Despite the promising results seen in multi-agent debate settings, little progress has been made in training models specifically to collaborate on tasks in real-time. In this paper, we investigate how this process could be entirely automated via multi-agent inference and training. We propose a decentralized system of LLMs assigned specific roles and structured around a central framework consisting of a generator, verifier, and refinement model operating sequentially. We first produce multiple interaction trajectories of the base model on math problems and use these generations as synthetic data for preference optimization. We then fine-tune individual models to become specialized in their roles by applying variants of Supervised Fine-tuning (SFT) and the Direct Preference Optimization (DPO) algorithm. We evaluate our approach on the GSM8k math problem-solving dataset, where both our multi-agent inference and trained multi-agent systems using Llama 3.1 8B achieve relative improvements of 4.8% and 11.2%, respectively, over the baseline performance. This demonstrates significant advances of our approach in the mathematical reasoning and cooperative capabilities of Llama 3.1 8B, enabling it to perform on par with the 8.6x larger state-of-the-art Llama 70B models. More generally, our work paves a way for multi-agent LLM training and gives promise to improve reasoning capabilities across a range of challenging tasks using synthetically generated data.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_2.png" alt="Escalation Risks from Language Models" class="research-image">
            <div class="research-content">
                <h3><a href="https://dl.acm.org/doi/abs/10.1145/3630106.3658942">Escalation Risks from Language Models in Military and Diplomatic Decision-Making</a></h3>
                <p><strong>Authors:</strong> Juan-Pablo Rivera, Gabriel Mukobi, Anka Reuel, Max Lamparth, <strong>Chandler Smith</strong>, Jacquelyn Schneider<br>
                <strong>Conference:</strong> FAccT 2024: Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency</p>
                <p>Our project investigates the potential risks and implications of integrating multiple autonomous AI agents within national defense strategies, exploring whether these agents tend to escalate or deescalate conflict situations. Through a simulation that models real-world international relations scenarios, our preliminary results indicate that AI models exhibit a tendency to escalate conflicts, posing a significant threat to maintaining peace and preventing uncontrollable military confrontations.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_3.png" alt="Evaluating Language Model Character Traits" class="research-image">
            <div class="research-content">
                <h3><a href="https://arxiv.org/abs/2410.04272">Evaluating Language Model Character Traits</a></h3>
                <p><strong>Authors:</strong> Francis Rhys Ward, Zejia Yang, Alex Jackson, Randy Brown, <strong>Chandler Smith</strong>, Grace Beaney Colverd, Louis Alexander Thomson, Raymond Douglas, Patrik Bartak, Andrew Rowan<br>
                <strong>Conference:</strong> Empirical Methods in Natural Language Processing (EMNLP) findings, 2024</p>
                <p>Language models (LMs) can exhibit human-like behaviour, but it is unclear how to describe this behaviour without undue anthropomorphism. We formalise a behaviourist view of LM character traits: qualities such as truthfulness, sycophancy, and coherent beliefs and intentions, which may manifest as consistent patterns of behaviour.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_4.png" alt=" Multi-agent Risk from Advanced AI Report" class="research-image">
            <div class="research-content">
                <h3><a href="">[Forthcoming] Multi-agent Risk from Advanced AI Report</a></h3>
                <p><strong>Authors:</strong> Lewis Hammond, <strong>Chandler Smith</strong>, Jesse Clifton, et al.<br>
                <strong>Expected Publication Date:</strong> November 2024</p>
                <p>In this report we take a first step in this direction by providing a taxonomy of risks that emerge, are much more challenging, or are qualitatively different in the multi-agent setting. We begin by identifying different failure modes in multi-agent systems based on the nature of the agents' goals and the intended behaviour of the system.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_5.png" alt="Secret Collusion" class="research-image">
            <div class="research-content">
                <h3><a href="https://www.lesswrong.com/posts/smMdYezaC8vuiLjCf/secret-collusion-will-we-know-when-to-unplug-ai">Secret Collusion: Will We Know When to Unplug AI?</a></h3>
                <p><strong>Authors:</strong> Christian Schroeder de Witt, Mikhail Baranchuk, Lewis Hammond, <strong>Chandler Smith</strong>, Sumeet Motwani</p>
                <p>We introduce the first comprehensive theoretical framework for understanding and mitigating secret collusion among advanced AI agents, along with CASE, a novel model evaluation framework.</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_7.png" alt="Conflict and Cooperation" class="research-image">
            <div class="research-content">
                <h3><a href="https://docs.google.com/presentation/d/1bIdWrTOHJSItNCEAztCE-62pqpR-LN70xCMAtTzRnj0/edit#slide=id.p">Conflict and Cooperation: An Exploration of Safety Concerns in Multi-Agent Systems</a></h3>
                <p><strong>Author:</strong> <strong>Chandler Smith</strong><br>
                <strong>Master's Thesis Defense Presentation</strong></p>
                <p>This work offers a broad examination of AI safety, delving into the technical foundations, algorithms, and pertinent failure modes associated with Multi-Agent Reinforcement Learning (MARL) and Cooperative Artificial Intelligence (AI).</p>
            </div>
        </div>
        <div class="research-item">
            <img src="images/research_8.png" alt="The State of AI in Maine" class="research-image">
            <div class="research-content">
                <h3><a href="https://ai.northeastern.edu/saim23">The State of AI in Maine</a></h3>
                <p><strong>Authors:</strong> Anna Fiorentino, Olivia Saucier, Tyler Lynch, <strong>Chandler Smith</strong></p>
                <p>Findings were drawn from 50 interviews conducted between July and December 2022. Interviewees were selected based on research by the reporting team and referrals from The State of AI in Maine Advisory Committee and many others. This report is a companion piece to The State of AI in Maine event, scheduled for Jan. 27, 2023. The report and event were created in parallel, with guidance from The State of Maine Advisory Committee and jointly funded by the Institute for Experiential AI and the Roux Institute at Northeastern University.</p>
            </div>
        </div>
    </div>
    <div class="ongoing-projects">
        <h2>Ongoing Projects</h2>
        <ul>
            <li><strong><a href="https://docs.google.com/document/d/1r4wEhAs3JIeEitV3PD0UD8n6R3YBOIYxKUiYRwTlAHA/edit?usp=sharing">Evaluation for Cooperative AI: Assessing Punitiveness in Multi-Agent Environments</a></strong></li>
            <li><strong>Eliciting Emergent Steganography</strong></li>
            <li><strong>Minimax Regret Algorithms as a performant alternative to utility maximization algorithms</strong></li>
            <li><strong>Error-Correcting AI Generated Steganography</strong></li>
            <li><strong>Quantifying Misalignment in Multi-Agent Systems</strong></li>
            <li><strong>AI Mediated Debate</strong></li>
        </ul>
    </div>
</body>
</html>